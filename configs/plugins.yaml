#
#  note: most of the config only has effect in the self test.
#  but, it's also how we reserve unique resources like ports, namespaces, schemas and container names for individual plugins.

#   - name: spark
#     description: "Spark cluster"
#     repo_path: ""
#     ports:
#        - number: 8010
#          description: "Ports used by the plugin, this is where you reserve ports."
#     namespaces:
#        - name: anything
#          description: "the unique resource for this plugin, redis databses, postgres schemas, etc. this is the palce to reserve them"
#     containers:
#        - spark-master

# note on ports: any port 8000-8999 will be tested as a web ui.

config:
  # nyi: but as this file grows we'll wantto split it into multiple files.
  # it could also be used to include plugins from other repos.
  # include_configs:
  #   - name: plugins
  #     description: "Extra configs to include in the checks. ('plugins' config itself is not actually required here, it's included as a sample)."

  plugins:
    - name: airflow
      description: "Airflow, multiple containers."
      ports:
        - number: 8001
          description: "Airflow web UI"
        - number: 5555
          description: "Celery flower"
      containers:
        - airflow-triggerer
        - airflow-worker
        - airflow-scheduler
        - airflow-webserver

    - name: devpi
      description: "Devpi, local python package index."
      namespaces:
        - name: root/tdfs
          description: "Where we put our packages."
      ports:
        - number: 8008
          description: "Devpi web UI"
      containers:
        - devpi

    - name: freeds-config
      description: "FREEDS config, the freeds config server"
      ports:
        - number: 8005
          description: "FREEDS config web UI"
      namespaces:
        - name: swagger-ui
          description: "Swagger UI for FREEDS config"
        - name: redoc
          description: "Redoc ui for FREEDS config"
        - name: api/configs
          description: "API endpoint for FREEDS configs"
      containers:
        - freeds-config

    - name: jupyter
      description: "Jupyter server to run spark notbooks and notebook runner container with papermill, intended for airflow dags."
      ports:
        - number: 8003
          description: "Papermill web UI"
      containers:
        - jupyter-spark

    - name: s3-minio
      description: "Minio S3, multiple containers."
      ports:
        - number: 8006
          description: "Minio web UI"
        - number: 9000
          description: "Minio S3 API - this port should be changed! vs code is sometimes using it"
      containers:
        - s3-minio
    - name: kafka
      description: "Kafka cluster in KRaft mode, 3 + 3 with kafdrop and akhq guis."
      ports:
        - number: 8015
          description: "Kafdrop UI"
        - number: 8016
          description: "akhq UI"
        - number: 29092
          description: "Kafka broker 1 PLAINTEXT"
        - number: 39092
          description: "Kafka broker 2 PLAINTEXT"
        - number: 49092
          description: "Kafka broker 3 PLAINTEXT"
      containers:
        - kafka-con-1
        - kafka-con-2
        - kafka-con-3
        - kafka-bro-1
        - kafka-bro-2
        - kafka-bro-3
        - akhq
        - kafdrop

    - name: postgres
      description: "PostgreSQL"
      namespaces:
        - name: airflow
          description: "Airflow schema"
        - name: spark_metastore
          description: "Hive metastore schema (spark)"
      ports:
        - number: 8002
          description: "PostgreSQL web UI"
        - number: 5432
          description: "PostgreSQL database"
      containers:
        - postgres

    - name: redis
      description: "Redis + Redis insight ui."
      namespaces:
        - name: 1
          description: "DB1: Airflow"
      ports:
        - number: 8007
          description: "Redis insight web UI"
        - number: 6379
          description: "Redis database"
      containers:
        - redis
        - redisinsight

    - name: spark
      description: "Spark cluster, capable of reaing gzip from s3 and storing delta tables on s3, using hive catalog."
      ports:
        - number: 8010
          description: "Spark master UI"
        - number: 8011
          description: "Spark worker 1 UI"
        - number: 8012
          description: "Spark worker 2 UI"
      containers:
        - spark-master
        - spark-worker-1
        - spark-worker-2
